# P2N â€” Planner Refactor & Next Milestones (Soft Sanitizer Path)
**Date:** 2025-10-19  
**Branch:** `clean/phase2-working`

---

## ðŸ”Ž Synopsis â€” What broke, why, and what weâ€™re changing

### The symptom (what you saw)
- **Stageâ€‘1 (o3â€‘mini)** produced excellent prose with File Search citations for CharCNN (AG News, DBpedia, Yelp, etc.). âœ…
- **Stageâ€‘2 (gptâ€‘4o)** sometimes returned:
  - **Invalid types** (numbers as strings) â†’ `E_PLAN_SCHEMA_INVALID`.
  - **Guardrail failure** when **any unregistered/blocked dataset** appeared (e.g., ImageNet / unknown license) â†’ `E_PLAN_GUARDRAIL_FAILED`.
  - With strict `json_schema` we then hit the platform rule: **schema must set `additionalProperties: false`** everywhere, otherwise the API rejects the responseâ€‘format.

### The root cause (why this is happening)
- We upgraded Stageâ€‘2 to strict structured outputs but:
  1) Some of our Pydanticâ€‘generated schema objects do **not** enforce `additionalProperties: false` at every nested level, which Responses API requires in strict mode.  
  2) The **license guardrail** was **hardâ€‘fail**: if Stageâ€‘1 mentions any dataset that isnâ€™t in our registry (or is large / restricted), Stageâ€‘2 dutifully encodes that â†’ guardrail rejects the whole plan even though we could simply **resolve** to an allowed dataset (e.g., AG News) or **omit** the restricted one.

### The ask (what we need now)
- **Unblock planning immediately** while we finish the full strictâ€‘schema migration:
  - Use a **soft sanitizer** in Stageâ€‘2 result handling to coerce types, strip extraneous keys, and **resolve/omit** outâ€‘ofâ€‘policy datasets **without failing the whole plan**.
  - Keep hard guardrails for truly dangerous items, but **downgrade** datasetâ€‘license failures to **rewrite + warn** so we still produce a runnable plan for covered datasets.

---

## ðŸ§­ Proposed refactor â€” â€œSoft Sanitizerâ€ in Stageâ€‘2

### What stays the same
- **Twoâ€‘stage planner**: o3â€‘mini â†’ gptâ€‘4o (Responses API).  
- Stageâ€‘1 uses **File Search**; Stageâ€‘2 formats to Plan v1.1.

### What changes
1) **Response sanitization layer** (post Stageâ€‘2):
   - **Type coercion**: `"10"` â†’ `10`, `"0.5"` â†’ `0.5`; `"true"`/`"false"` â†’ booleans.
   - **Key pruning**: drop unknown fields per schema (â€œ`additionalProperties: false`â€ effect).
   - **Dataset resolution**:
     - If **not** in registry or **blocked**: remove that claim/dataset from plan and add a **`warnings[]`** entry.
     - Prefer **canonical registry name** (e.g., `"AG News"` â†’ `"ag_news"`).
   - **Justifications fixup**: if Stageâ€‘1 prose provided paper quotes, move them into the required nested `justifications.{dataset|model|config}.{quote,citation}` shape.
   - **If after pruning no datasets remain** â†’ fail closed with a **clear typed error** telling the user how to proceed (â€œadd DBpedia to registry or choose a covered datasetâ€).

2) **Guardrail policy tweak**:
   - Dataset license / registry issues â†’ **rewrite + warn**, not fatal.  
   - Still fatal for: prohibited tools, missing mandatory sections, or empty final plan.

3) **Nonâ€‘strict Stageâ€‘2 request** (temporary):
   - Use `response_format={"type": "json_object"}` for Stageâ€‘2 to maximize yield, and let the **sanitizer** enforce (and document) strictness **postâ€‘hoc**.
   - Keep a feature flag to reâ€‘enable **strict `json_schema`** once we finish the schema pass adding `additionalProperties: false` recursively.

> Result: we ship runnable plans for **covered datasets** immediately, and the pipeline wonâ€™t stall just because Stageâ€‘1 mentions an unregistered dataset.

---

## âœ… What we already fixed / verified
- **Phaseâ€‘2 Microâ€‘Milestone 1**: notebooks load **real datasets** (HF/torchvision/sklearn) with lazy caching; ImageNet intentionally falls back to synthetic.  
- **Supabase**: SDK update chaining; artifacts in `plans` bucket; duplicate upload handling; claims persistence is **idempotent** with deleteâ€‘thenâ€‘insert.

---

## ðŸ§© What weâ€™ll implement now (Fâ€‘series milestones)

### F1 â€” Soft Sanitizer in Stageâ€‘2 (NOW)
**Goal:** Always return a runnable plan for covered datasets; coerce types; prune extras; resolve/omit restricted datasets; add `warnings[]`.

**Changes**
- `api/app/routers/plans.py`
  - After Stageâ€‘2, run `sanitize_plan(plan_json, registry, policy)`:
    - Coerce numbers/booleans/nulls.
    - Remove unknown keys (simulate `additionalProperties: false`).
    - Map datasets to canonical registry IDs; drop and warn blocked/unknown.
    - Normalize justifications `{quote,citation}` from Stageâ€‘1 prose.
  - If `datasets[]` becomes empty â†’ `E_PLAN_NO_ALLOWED_DATASETS` with actionable message.
- **Logs & SSE**: add `stage_update: "sanitize_start"|"sanitize_done"`, include `warnings_count`.

**Acceptance**
- CharCNN (AG News) â†’ PASS (no failure if DBpedia also appears in prose).
- ResNet (CIFARâ€‘10) â†’ PASS.
- DenseNet (CIFARâ€‘100) â†’ PASS.
- MobileNetV2 (ImageNet) â†’ plan excludes ImageNet, warns, still materializes.

---

### F2 â€” Registryâ€‘only Planner Mode (Prompt + Allowlist)
**Goal:** Reduce Stageâ€‘2 work by **constraining Stageâ€‘1** to registry items.

**Changes**
- Planner system prompt: â€œ**Only** plan for datasets present in our registry and **present in the claims**; omit others.â€
- Optional allowlist passed in request (`claims[].dataset`) is included in Stageâ€‘1 input (â€œpick from these onlyâ€).

**Acceptance**
- Stageâ€‘1 prose focuses on covered datasets; Stageâ€‘2 receives cleaner content; sanitizer produces 0â€“1 warnings.

---

### F3 â€” DBpediaâ€‘14 & SVHN (Registry + Tools Sync)
**Goal:** Cover more CharCNN/fastText and DenseNet claims.

**Changes**
- `dataset_registry.py`: add `dbpedia_14` (HF) & `SVHN` (torchvision).
- Sync **function tools** dataset map (if still enabled) to registry entries.

**Acceptance**
- CharCNN + DBpedia â†’ emits `load_dataset("dbpedia_14")`.
- DenseNet + SVHN â†’ emits `torchvision.datasets.SVHN`.

---

### F4 â€” TorchTextCNNGenerator (Phaseâ€‘3 start)
**Goal:** First â€œsmartâ€ model generator for NLP; runnable within 20â€‘minute CPU budget.

**Changes**
- `generators/model.py`: `TorchTextCNNGenerator`
  - Small embeddings + 1â€“2 conv filter widths + maxâ€‘pool + linear head.
  - Deterministic seeding; subâ€‘sampled dataset; metrics logged to `metrics.json`.
- `factory.py`: map plan.model ~ â€œtextcnnâ€ or dataset family to this generator.

**Acceptance**
- Materialized SSTâ€‘2 notebook trains a tiny CNN and logs accuracy; â‰¤ 20 min CPU on dev laptop.

---

### F5 â€” Runner Shim & Metrics (M3.1)
**Goal:** Execute notebooks locally (CPU) and upload `metrics.json`, `logs`, `events` to `assets` bucket; SSE live progress.

**Changes**
- `runs/executor_local.py` and `routers/runs.py`
- Use `papermill` or `nbclient` + time/memory limits; stream console to SSE.

**Acceptance**
- `/runs` produces artifacts; deterministic metrics between runs (within tolerance).

---

### F6 â€” Gap Analyzer (M3.2)
**Goal:** Compare observed vs. claimed metrics; store to `evals` and surface in UI.

**Acceptance**
- Paper page shows claimed vs. observed; CSV export works.

---

## ðŸ§ª Test plan (thin but strong)

- **Unit**: sanitizer coercion (`"10"`â†’10), key pruning, dataset mapping (aliasesâ†’canonical), quote extraction shim â†’ nested justifications.  
- **Integration**: CharCNN(AGâ€‘News), ResNet(CIFARâ€‘10), DenseNet(CIFARâ€‘100), MobileNetV2(ImageNet) â†’ **no fatal guardrail**; sanitized plans materialize.  
- **Smoke**: ingestâ†’extractâ†’planâ†’materialize for one NLP + one vision in CI within 10 minutes.

---

## ðŸ“£ Developer notes

- Keep **Stageâ€‘2 strict mode** behind a feature flag; once we propagate `additionalProperties:false` through the Pydantic schema tree, flip back to strict Responses `json_schema`.  
- Prefer **warnings and rewrites** over hard failures for dataset licensing unless the user explicitly requests â€œstrict licensingâ€.  
- Document `warnings[]` in plan payload; surface to UI so users understand substitutions/omissions.

---

## ðŸ§° Context rehydration prompt (for a fresh Claude Code chat)

> **Role:** You are the PromptOps/Code copilot for the P2N project.  
> **Branch:** `clean/phase2-working`.  
> **Authoritative docs:**  
> â€“ `docs/Claudedocs/P2N_MILESTONE_UPDATE__PLANNER_REFACTOR_2025-10-19.md` (this file)  
> â€“ `docs/Claudedocs/SeedSetup/INGEST_GUIDE.md`  
> â€“ `api/app/materialize/generators/dataset_registry.py`  
> â€“ `api/app/routers/plans.py`  
>  
> **Invariants:** Use **OpenAI Responses API**; File Search uses the paperâ€™s `vector_store_id`; planner is twoâ€‘stage (o3â€‘mini â†’ gptâ€‘4o).  
>  
> **First actions (donâ€™t ask, do):**  
> 1) Implement **F1 Soft Sanitizer** in `plans.py` (coercion, pruning, dataset resolve/omit with warnings, justifications shim).  
> 2) Add `stage_update` logs for `sanitize_start/done`.  
> 3) Add unit tests for sanitizer functions.  
>  
> **Acceptance:** CharCNN(AGâ€‘News) & ResNet(CIFARâ€‘10) plans pass and materialize; MobileNetV2(ImageNet) warns and omits ImageNet but plan still materializes.  
>  
> **Do not** modify extractor. Avoid adding new tools; prefer registry + sanitizer.

---

## ðŸ“ File path to commit

Place this file at:  
`docs/Claudedocs/Milestones/P2N_MILESTONE_UPDATE__PLANNER_REFACTOR_2025-10-19.md`

> **Archive relocation (2025-10-19):** This document now lives at
> `docs/history/2025-10-19_P2N_Planner_Refactor.md`. Keep the original instruction
> above for historical accuracy.
