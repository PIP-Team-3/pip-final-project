# P2N — AGENTS.md (v1.1 • Detailed, With Reviewer Prompts)

> **Paper‑to‑Notebook Reproducer (P2N)** turns a research paper into a small, deterministic experiment, runs it in a sandbox, and explains the result in both Pro‑Mode charts and a kid‑friendly Storybook.  
> This document is the **single source of truth** for agent roles, inputs/outputs, tools, guardrails, orchestration, security, and acceptance criteria.  
> It also includes **Claude 4.5 Code Reviewer context hydration** and **review/test checklists** under each section.

**Audience**: engineers, product, AI/agents implementers, reviewers.  
**Scope**: contracts & procedures only—no executable code in this file.  
**Change control**: any change to roles/tools/guardrails/event schemas must update this doc.

---

## 0) How to use this doc

- **Builders (Codex users)**: follow the *Prompt Pack* tasks (P‑A0 → P‑A18) referenced herein.  
- **Reviewers (Claude 4.5)**: before each milestone, copy the “**Context Hydration**” block into Claude, then run the **Test Checklist** items against the codebase generated by Codex.  
- **PM/Stakeholders**: skim 1–4 for vision/scope; skim 13–15 for delivery/acceptance.

### Claude Reviewer — Context Hydration (global, paste once at start)
> **You are Claude 4.5 acting as code reviewer and test runner for the P2N project.**  
> Your job: validate that Codex outputs implement the contracts in this AGENTS.md.  
> You must: (1) read files; (2) reason about types, schemas, and policies; (3) invoke project scripts documented by the repo; (4) report exact failures and the contract they violate; (5) recommend precise fixes.  
> Do **not** write business logic unless the prompt explicitly asks.  
> Prioritize: safety (guardrails tripwires), reproducibility (determinism), and policy caps (tool budgets).  
> Acceptable artifacts are listed under each test. If a contract is ambiguous, ask for the specific clause ID from this doc (e.g., “§6.3‑G2”).

---

## 1) Vision, Non‑Goals, Success Metrics

**Vision**: P2N ingests a paper (PDF/URL), extracts experimental **claims** with citations, plans a minimal reproducible **experiment** to verify those claims, generates a deterministic **notebook + environment**, **executes** it in a sandbox (CPU‑only, no network), streams events to the UI, and produces an **explainable report** plus a **Kid‑Mode Storybook** that updates with the final score.

**Non‑Goals (MVP)**  
- Multi‑GPU or >20‑min training runs; arbitrary shell access; broad generalized web crawling.  
- Handling private/PII datasets without explicit compliance gates.

**Success Metrics**  
- **M‑A (no training)**: 2 seed papers complete Upload → Extract → Plan → Storybook in < 60s each; guardrails block unsafe plans.  
- **M‑B (eval‑only)**: metrics computed in < 5m; gap reported; artifacts persisted.  
- **M‑C (full run)**: ≤ 20‑min CPU; success ≥ 85%; reproducible metrics; artifacts downloadable.

### Claude Reviewer — Test Checklist (Vision)
- Confirm `docs/AGENTS.md` present and matches this version header “v1.1 Detailed”.  
- Confirm repo includes a **non‑code** product overview (README/Docs) consistent with this section.  
- Verify acceptance criteria for M‑A/M‑B/M‑C are mirrored in CI or scripts (even as TODOs).

**Artifacts**: links to README sections; CI/config notes referencing milestones.

---

## 2) User Journeys & UX Guarantees

**Primary flow**: Upload PDF → Extract claims (with citations) → Review Plan → Storybook (static) → (later) Run → Gap report → Artifacts.  
**Fallbacks**: Low extractor confidence → manual claim editor; dataset ambiguity → top‑3 resolver with licenses; private data → eval‑only; license block → user override with justification.

**UX Guarantees**  
- Always show **citations** for claims/plan justifications.  
- Always stream **progress** during Extract/Plan.  
- Always surface **remediate‑able** messages when blocked by guardrails.

### Claude Reviewer — Context Hydration (User Journeys)
> You are validating UX guarantees for Extract/Plan/Storybook flows. Ensure citations and streaming are present; verify that blocked flows include remediation text, not generic errors.

### Claude Reviewer — Test Checklist (User Journeys)
- Inspect web routes/pages: Upload, Claims/Plan view, Storybook, Pro Report.  
- Trigger a guardrail failure (e.g., plan missing license) and verify **remediation text**.  
- Confirm SSE renders progress during Extract and Plan.

**Artifacts**: screenshots or console logs showing streaming and remediation text.

---

## 3) Agentic Architecture & Choices

**Pattern**: **Thin agents, thick tools**. Each agent has a narrow role + typed output. Tools (hosted + local) do the heavy lifting.  
**Orchestrator**: Deterministic **state machine** in the API. Agents never run arbitrary code; they propose JSON; backend validates before acting.  
**SDK**: OpenAI **Agents SDK** (agents, tools, guardrails/tripwires, streaming, tracing). Handoffs/sessions are optional and bounded.

**Why not multi‑agent chat?** Reproducibility and safety: deterministic DAG > open‑ended loops.

### Claude Reviewer — Context Hydration (Architecture)
> Validate that the code follows “thin agents, thick tools”: agents only produce structured outputs; the backend executes tools and validation. Verify no agent invokes arbitrary shell or training.

### Claude Reviewer — Test Checklist (Architecture)
- Read `/api/app/agents/*` and confirm outputs are typed, not free‑form text.  
- Confirm tool calls are registered with schemas; hosted tools usage is gated by caps.  
- Verify there is a state machine (or equivalent) sequencing stages and handling retries/timeouts.

**Artifacts**: file paths and excerpts showing typed outputs and policy caps (no code snippets required—cite filenames and line ranges).

---

## 4) Roles Summary (Cards)

We define **five MVP agents**. Each card lists: role, inputs, outputs, tools, budgets, timeouts, guardrails, failure cases, telemetry.

### 4.1 Claim & Method Extractor
- **Role**: Extract **verifiable** experimental claims (dataset, split, metric, value, units) with **citations** and **confidence**.  
- **Inputs**: `paper_id`; structured sections; **File Search index id**.  
- **Outputs**: array of Claim objects (dataset_name, split, metric_name, metric_value, units, method_snippet, source_citation, confidence∈[0,1]).  
- **Tools**: Hosted **File Search** (grounding); **no planning**.  
- **Budgets**: model tokens per run; File Search ≤ 10 calls.  
- **Timeout**: ≤ 12s p95.  
- **Guardrails**: citations required; no invented values; unknown→null; confidence threshold.  
- **Failure cases**: low confidence; missing citations → route to manual review.  
- **Telemetry**: #claims, avg confidence, #file‑search calls, p95 latency.

### Claude Reviewer — Context Hydration (Extractor)
> Validate Extractor produces structured claims with citations, uses File Search within caps, and never proposes plans.

### Claude Reviewer — Test Checklist (Extractor)
- Run Extract endpoint with a seed PDF; check output includes citations and confidence.  
- Force a low‑confidence case (e.g., malformed PDF) → confirm manual review path.  
- Confirm File Search calls ≤ cap; on cap exceed, a policy error is raised with remediation.

**Artifacts**: API response sample (redacted), log showing policy cap enforcement, trace link.

---

### 4.2 Reproduction Planner
- **Role**: Emit **Plan v1.1** obeying CPU ≤ 20 min and correct metric semantics.  
- **Inputs**: claims; dataset catalog; policy (budget, license allow‑list).  
- **Outputs**: Plan object: version, targets, resources.datasets (canonical), env, run (seed, model, epochs), metrics, viz, explain, timeouts, artifacts, `justifications{field→paper quote}`.  
- **Tools**: Function tools (`dataset_resolver`, `license_checker`, `budget_estimator`); Hosted **Web Search** for canonical dataset/licensing pages.  
- **Budgets**: Web Search ≤ 5 calls.  
- **Timeout**: ≤ 10s p95.  
- **Guardrails**: (G1) schema valid; (G2) license allowed; (G3) budget ≤ 20 min; (G4) one primary metric. Violations → **tripwire**.  
- **Failure cases**: missing dataset; license block; over‑budget; multi‑metric conflicts.  
- **Telemetry**: validation pass rate; calls count; p95 latency.

### Claude Reviewer — Context Hydration (Planner)
> Validate Planner returns a Plan that passes schema/license/budget guardrails. If Web Search is enabled, ensure calls ≤ cap and citations point to canonical dataset/license pages.

### Claude Reviewer — Test Checklist (Planner)
- Provide a claim with ambiguous dataset → ensure `dataset_resolver` picks canonical or returns candidates for UI.  
- Force a license conflict → verify **tripwire** stops execution with remediation.  
- Inflate epochs to exceed budget → Planner must auto‑downscale or fail with remediation; in all cases guardrail triggers are visible in traces.

**Artifacts**: Plan JSON (redacted) validating fields; trace with guardrail trigger; admin counter showing Web Search calls.

---

### 4.3 EnvSpec Builder
- **Role**: Propose deterministic **environment spec** (pinned versions, content hash) using CPU‑only.  
- **Inputs**: Plan.  
- **Outputs**: `env_spec` (python version, pinned libs, lock hash).  
- **Tools**: Function tools (`env_lock_builder`, `env_policy_check`); optional **Code Interpreter** for **preflight** CSV schema/row‑count checks only.  
- **Budgets**: Code Interpreter ≤ 60s.  
- **Guardrails**: forbid GPU/CUDA, forbid network‑dependent packages; lockfile present.  
- **Failure cases**: conflicting pins; unsafe package; excessive interpreter runtime.  
- **Telemetry**: lock generation time; policy violations count.

### Claude Reviewer — Context Hydration (EnvSpec)
> Ensure env is CPU‑only with pinned versions, and any “preflight” Code Interpreter run is bounded and non‑training. Verify unsafe packages are blocked.

### Claude Reviewer — Test Checklist (EnvSpec)
- Submit a plan requiring CUDA → ensure guardrail blocks with remediation.  
- Trigger Code Interpreter path with a mock CSV → verify single ≤60s run; exceeding time causes policy error.

**Artifacts**: env spec sample (names/versions), logs showing policy check results, trace of interpreter run.

---

### 4.4 CodeGen (Design)
- **Role**: Output a **notebook design contract** (cells, actions, emissions), not executable code.  
- **Inputs**: Plan + env_spec.  
- **Outputs**: design spec including: “set seeds”, “print versions”, “no GPU/no network”, event emissions (`metric_update`, `sample_pred`, `progress`, `stage_update`, `log_line`), and final `metrics.json`.  
- **Guardrails**: design must include exit‑non‑zero on metric failure; bounded logging.  
- **Failure cases**: missing event emissions; no metrics write; implicit GPU or network.

### Claude Reviewer — Context Hydration (CodeGen‑Design)
> Verify the design spec contains all required cells/actions/emissions and explicitly forbids GPU/network use. No executable code must be generated at this stage.

### Claude Reviewer — Test Checklist (CodeGen‑Design)
- Confirm required sections exist: seeds, versions, dataload, train/eval, metrics write, event emissions.  
- Confirm “failure on metric missing” is specified.

**Artifacts**: design spec document; checklist mapping required sections to spec entries.

---

### 4.5 Kid‑Explainer
- **Role**: Produce **Storyboard JSON** (5–7 pages) in grade‑3 language with alt‑text.  
- **Inputs**: claim + plan summary; later the final observed metric.  
- **Outputs**: storyboard with pages: title, big claim, recipe, test, live bar, meaning, final two‑bar score; all visuals have alt‑text.  
- **Guardrails**: reading level constraints; prohibited content filters; motion‑reduced variant noted.

### Claude Reviewer — Context Hydration (Kid‑Explainer)
> Validate storyboard JSON structure, alt‑text presence, and reading‑level intent. Confirm a “live results” slot is present.

### Claude Reviewer — Test Checklist (Kid‑Explainer)
- Ensure 5–7 pages exist with required semantics.  
- Verify alt‑text is present and non‑empty.  
- Confirm a placeholder for “Our vs Paper” is included.

**Artifacts**: storyboard JSON excerpt and UI screenshot (if available).

---

## 5) Tools Registry (Hosted & Function)

**Hosted Tools (OpenAI platform)** — enable if available in your account; otherwise disable and rely on function tools.  
- **File Search**: retrieves grounded passages from uploaded PDF(s)/supplementals for Extractor/Planner. **Caps**: ≤ 10 calls/run.  
- **Web Search**: fetches canonical dataset/license pages for Planner. **Caps**: ≤ 5 calls/run.  
- **Code Interpreter (preflight)**: sandboxes quick CSV/schema checks for EnvSpec only. **Caps**: ≤ 60s/run.

**Function Tools (local)** — arguments validated; allow‑lists enforced.  
- `dataset_resolver(name|alias) → {canonical_id, source, license, size, checksum}`  
- `license_checker(dataset_id) → {allowed: bool, reason}`  
- `budget_estimator(plan) → {minutes_estimate, warnings[]}`  
- `env_lock_builder(plan) → {pins[], hash}`  
- `env_policy_check(env_spec) → {ok: bool, violations[]}`  
- `sandbox_submit(design_spec) → {run_id}` (no direct execution by agent)  
- `gap_calculator(claimed, observed) → {gap_pct}`

**Policy**: server‑enforce caps; violations cause typed policy errors; all tool usage appears in traces.

### Claude Reviewer — Context Hydration (Tools)
> Validate hosted tools are registered only for the agents allowed here and that function tools enforce schemas and allow‑lists. Verify caps are enforced and logged.

### Claude Reviewer — Test Checklist (Tools)
- Attempt to over‑call File/Web Search and verify caps stop execution gracefully.  
- Pass invalid args to a function tool → receive typed validation error.  
- Confirm traces list tool invocations with agent context.

**Artifacts**: policy config snapshot (values only), error messages, trace links.

---

## 6) Guardrails & Tripwires

**Input Guardrails**  
- File type/size; PDF parse confidence threshold; disallowed content.  
- Dataset BYOD manifests: schema + checksums + license.

**Output Guardrails**  
- **Plan**: schema validity, budget ≤ 20 min, license allow‑list, single primary metric.  
- **EnvSpec**: CPU‑only; no network; pinned versions; lock hash present.  
- **Design**: seeds set; versions printed; event protocol; metrics write required.  
- **Storyboard**: page count; alt‑text non‑empty; reading‑level constraint.

**Tripwire Behavior**  
- On violation, raise a **typed** error; stop orchestration; emit remediation text; record in trace.  
- UI surfaces friendly next actions (pick canonical dataset; reduce epochs; accept license; etc.).

### Claude Reviewer — Context Hydration (Guardrails)
> Induce violations and verify tripwires: Plan schema error, license block, budget exceed, EnvSpec unsafe package, design missing event emissions.

### Claude Reviewer — Test Checklist (Guardrails)
- Attempt Plan without license → tripwire fires with remediation.  
- Inflate epochs to exceed budget → tripwire fires OR auto‑downscale logs fidelity warning.  
- Add CUDA‑only package → EnvSpec guardrail blocks.  
- Remove metrics write from design → guardrail blocks.

**Artifacts**: error logs, trace entries referencing violations, UI screenshots showing remediation.

---

## 7) Orchestration & State Machine

**States**: `INGESTED → EXTRACTED → PLANNED → MATERIALIZED → RUNNING → COMPLETED|FAILED`  
**Rules**:  
- Each state transition requires validated output.  
- Retries: bounded with backoff.  
- Idempotency: content hashes for PDF/plan/env; cache agent outputs by hash.  
**Failure handling**: typed errors; never partial side‑effects.  
**Streaming**: run‑streamed token + item events → SSE mapping.  
**Handoffs**: optional tools (Extractor→HumanReview→Planner); disabled by default.

### Claude Reviewer — Context Hydration (Orchestration)
> Ensure deterministic transitions, cached outputs by hash, and bounded retries. Verify SSE receives token and item events across Extract/Plan.

### Claude Reviewer — Test Checklist (Orchestration)
- Re‑run the same paper/plan → cached path taken; no duplicate writes.  
- Verify invalid transitions are refused (e.g., Run before Plan).  
- Confirm SSE emits stage transitions and progress consistently.

**Artifacts**: logs showing cache hits, rejected invalid transitions, SSE transcripts.

---

## 8) Data Model, Storage & RLS

**Tables** (MVP): `profiles`, `papers`, `paper_sections`, `claims`, `datasets`, `plans`, `runs`, `run_metrics`, `run_series`, `run_events`, `assets`, `storyboards`.  
**Keys**: UUID PKs for top‑level; BIGINT identity for high‑churn (`run_*`, `paper_sections`).  
**Timestamps**: TIMESTAMPTZ everywhere.  
**Cascades**: deleting a paper cascades to sections/claims/plans/storyboards/assets (and runs in MVP).  
**Indexes**: FKs; `(run_id, ts)` on events; `(run_id, step)` on series; GIN on `plans.plan_json`.  
**RLS**: owner read/write; optional `is_public` read on `papers`/`assets`; join‑based read for `run_*` via run ownership.  
**Retention**: thin time‑series in DB; raw logs/notebooks in Storage.

### Claude Reviewer — Context Hydration (Data & RLS)
> Validate schema files match this model, RLS is enabled on user tables, and indexes exist. Verify that client code never holds service keys.

### Claude Reviewer — Test Checklist (Data & RLS)
- Ensure `schema.sql`, `rls.sql`, and `seed.sql` exist and execute cleanly.  
- Confirm RLS policies restrict access as described.  
- Verify `.env.example` is present; no secrets committed; client uses only anon key.

**Artifacts**: SQL execution logs, RLS status screenshots, grep results for secret misuse (filenames only).

---

## 9) Streaming & Event Protocol

**Inbound (SDK)**: token deltas + run‑item events (tool_started, tool_output, message_completed, agent_updated).  
**Outbound (SSE)**:  
- `progress{percent, stage}`  
- `stage_update{stage, epoch?, of?}`  
- `log_line{text, level}`  
- `metric_update{metric, value, split, step}`  
- `sample_pred{id, true, pred, thumb}`

**Reliability**: SSE reconnects; backpressure; bounded log sizes; auto‑close on completion.

### Claude Reviewer — Context Hydration (Streaming)
> Verify mapping from SDK events to SSE payloads is complete and consistent. Ensure SSE closes properly, and logs are bounded.

### Claude Reviewer — Test Checklist (Streaming)
- Observe Extract/Plan runs: token + item events arrived; SSE payloads match the mapping.  
- Force high‑volume logs → verify bounds and no memory leaks.

**Artifacts**: SSE transcript samples, mapping table in code (filename/section).

---

## 10) Tracing & Observability

**Tracing**: enabled by default; each agent run emits generations, tool calls, guardrails, and handoffs; traces are linked from Admin.  
**Metrics**: time‑to‑first‑signal, success rate, guardrail triggers, tool call counts; token usage by agent.  
**Logging**: structured; run id, agent id, policy caps; no secrets.

### Claude Reviewer — Context Hydration (Tracing)
> Confirm tracing is on; administrators can click from runs to traces. Verify token usage by agent is logged.

### Claude Reviewer — Test Checklist (Tracing)
- Ensure each agent’s run has a trace URL.  
- Confirm guardrail violations appear in traces with the correct labels.  
- Check that token counts per run are recorded.

**Artifacts**: trace links (redacted), admin UI screenshot, log lines referencing tokens.

---

## 11) Security & Compliance

**Secrets**: service role and OpenAI keys are server‑only; client has anon key.  
**Sandbox**: network‑off; datasets prefetched; 2 CPU / 6 GB / 25‑min cap.  
**Licenses**: license allow‑list; block incompatible sources unless user overrides with justification stored in DB.  
**BYOD**: manifest + checksums + license; PII blocked unless a compliance gate is passed (out‑of‑scope for MVP).

### Claude Reviewer — Context Hydration (Security)
> Validate secrets placement, sandbox policy, and license gates. Inspect BYOD manifest validation if present.

### Claude Reviewer — Test Checklist (Security)
- Search for service keys in client bundle (should be absent).  
- Ensure sandbox launch config forbids network by default.  
- Trigger a license‑blocked dataset and confirm stop + recorded reason.

**Artifacts**: build output inspection notes, sandbox config file path, policy logs.

---

## 12) Costs, Budgets & Admin Caps

**Models**: one “mini” default for all agents; bump selectively as needed.  
**Hosted tools**: caps per run and per day; admin panel shows counters and remaining budget.  
**Reports**: per‑agent token usage; per‑tool calls; monthly budget alerts configured.

### Claude Reviewer — Context Hydration (Costs)
> Verify caps are enforced and visible; costs are estimated per run; monthly budget alerts documented.

### Claude Reviewer — Test Checklist (Costs)
- Exceed a tool cap → blocked with human‑readable message and trace label.  
- Confirm admin usage page increments appropriately per run.

**Artifacts**: counter values before/after, policy error message, screenshot of admin page.

---

## 13) Milestones & Acceptance

**M‑A (Agents + Hosted Tools + Streaming)**  
- Upload → Extract (citations) → Plan (validated) → Storybook (static).  
- Traces show tool calls + guardrails; SSE shows live progress.

**M‑B (Evaluate‑only)**  
- Metrics computed on small eval set; gap reported; artifacts stored.

**M‑C (Full CPU run, Top‑3)**  
- ≤ 20‑min runs; reproducible outputs; artifacts downloadable; Pro report with citations/deltas.

### Claude Reviewer — Context Hydration (Milestones)
> At each milestone, run the end‑to‑end smoke script and compare outcomes to acceptance criteria in this section.

### Claude Reviewer — Test Checklist (Milestones)
- M‑A: run both seed papers through to Storybook; confirm traces/SSE and guardrails.  
- M‑B: run eval‑only; verify metrics.json and gap are correct.  
- M‑C: run a full job; verify time budget and artifact bundle completeness.

**Artifacts**: smoke run logs, metrics.json excerpts, gap calculation screenshot.

---

## 14) PR Process & Quality Gates

- **One Prompt → One PR** (P‑A0 … P‑A18).  
- **CI checks**: schema lint, guardrail unit tests, SSE smoke, no secrets, RLS enforced.  
- **Review rubric**: agent role clarity; structured outputs; caps & guardrails; tracing hooks; documentation updated.

### Claude Reviewer — Context Hydration (PRs)
> Evaluate PRs against the rubric; block merges if guardrails or caps are missing, or if docs aren’t updated.

### Claude Reviewer — Test Checklist (PRs)
- Check that PR references which clause(s) in this AGENTS.md are implemented.  
- Confirm CI includes at least schema/guardrail/SSE checks.

**Artifacts**: CI run summary, PR description screenshot.

---

## 15) Appendices (Reference)

**A) Event Registry** — `metric_update`, `sample_pred`, `progress`, `stage_update`, `log_line` (see §9).  
**B) Budget Policies** — default 20‑min run time; 25‑min hard cap in sandbox.  
**C) License Levels** — permissive first; gated sources require explicit override.  
**D) Dataset Alias Examples** — SST‑2 (GLUE/SST‑2), CIFAR‑10 (torchvision/hf).  
**E) Definitions** — Agent, Tool, Guardrail, Tripwire, Handoff, Sandbox, Storybook, Reproduction Gap.

### Claude Reviewer — Context Hydration (Appendix)
> Use Appendix as reference when checking field names, events, and budgets.

---

## 16) Cross‑reference to Prompt Pack (what Codex is asked to implement)

- **P‑A0**: SDK bootstrap & tracing; **P‑A1**: tools registry; **P‑A2**: hosted tools; **P‑A3**: agents + guardrails; **P‑A4**: orchestrator; **P‑A5**: streaming bridge; **P‑A6**: tracing surface; **P‑A7**: handoffs; **P‑A8**: file ingest + File Search index; **P‑A9**: planner policies; **P‑A10**: admin caps; **P‑A11**: seed tests; **P‑A12–A18**: DB/RLS/seed, demo UI, smoke tests, etc.

### Claude Reviewer — Context Hydration (Prompt Pack)
> For each PR, read the corresponding Prompt Pack task and confirm deliverables and acceptance are satisfied—refer back to the specific clauses in this AGENTS.md.

---

## 17) Change Log

- **v1.1 (this file)** — Expanded detail + per‑section reviewer prompts/tests; clarified hosted tool caps; enumerated guardrail sets and telemetry.  
- **v1.0** — Initial agents contract and minimal reviewer notes.

---

**End of document — keep this file in `/docs/AGENTS.md` and update with every contract change.**

